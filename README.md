# ğŸš€ [Quantization Fundamentals with Hugging Face](https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face/)

ğŸ’» Welcome to the "Quantization Fundamentals with Hugging Face" course! Instructed by Younes Belkada and Marc Sun, Machine Learning Engineers at Hugging Face, this course will equip you with the knowledge and skills to compress and optimize generative AI models using quantization techniques.


## Course Summary
In this course, you'll dive into the fundamentals of model quantization, a crucial technique for compressing and optimizing generative AI models like large language models (LLMs). Here's what you can expect to learn and experience:

1. âš™ï¸ **Linear Quantization**: Learn how to quantize any open-source model with linear quantization using the Quanto library, gaining an understanding of its implementation and applicability to various types of models, including LLMs and vision models.
2. ğŸ”„ **Downcasting**: Explore downcasting, another form of quantization, with the Transformers library, enabling you to load models in about half their normal size in the BFloat16 data type.
3. ğŸ’¡ **Hands-on Practice**: Practice quantizing open-source multimodal and language models, gaining practical experience in compressing and optimizing generative AI models.

## Key Points
- ğŸ¯ Gain proficiency in compressing models using the Hugging Face Transformers library and the Quanto library.
- ğŸ§  Understand the concepts and implementation of linear quantization, a powerful method for compressing models while maintaining performance.
- ğŸ› ï¸ Apply quantization techniques to optimize and make generative AI models more accessible and efficient.

## About the Instructors
ğŸŒŸ **Younes Belkada** and **Marc Sun** are both Machine Learning Engineers at Hugging Face, bringing extensive expertise in model compression and optimization to guide you through this course.

ğŸ”— To enroll in the course or for further information, visit [deeplearning.ai](https://www.deeplearning.ai/short-courses/).
